{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check if page downloaded successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.dli.pa.gov/Individuals/Workforce-Development/warn/notices/Pages/default.aspx\"\n",
    "page = requests.get(url)\n",
    "page.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## webpage url parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "data = soup.findAll('div',attrs={'class':'ms-rtestate-field'})\n",
    "month_links = []\n",
    "for div in data:\n",
    "    links = div.findAll('a')\n",
    "    for a in links:\n",
    "        month_links.append(\"https://www.dli.pa.gov/\"+a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(month_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#month_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "February-2020 is done!\n",
      "January-2020 is done!\n",
      "December-2019 is done!\n",
      "November-2019 is done!\n",
      "October-2019 is done!\n",
      "September-2019 is done!\n",
      "August-2019 is done!\n",
      "July-2019 is done!\n",
      "June-2019 is done!\n",
      "May-2019 is done!\n",
      "April-2019 is done!\n",
      "March-2019 is done!\n",
      "February-2019 is done!\n",
      "January-2019 is done!\n"
     ]
    }
   ],
   "source": [
    "def get_df(url):\n",
    "    #\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    table_data = soup.findAll('td')\n",
    "    #\n",
    "    company_name = []\n",
    "    company_address = []\n",
    "    country_l = []\n",
    "    employees_affected_l = []\n",
    "    effective_date_l = []\n",
    "    closure_or_layoff_l = []\n",
    "    \n",
    "    #\n",
    "    error_link = []\n",
    "    error_name = []\n",
    "\n",
    "    for ele in table_data:#[10:11]:\n",
    "        data = ele.find_all('p')\n",
    "        if data:\n",
    "            try:\n",
    "                name = data[0].find('strong').text\n",
    "            except: \n",
    "                continue\n",
    "            if len(data) != 2:\n",
    "                error_name.append(name)\n",
    "                error_link.append(url)\n",
    "                continue\n",
    "            if name in company_name:\n",
    "                continue\n",
    "            ad_list = []\n",
    "            #delete the name part\n",
    "            ad_list.append(\", \".join([ele.strip() for ele in re.findall(r'\\>([^\\>]*)\\<', str(data[0]).split(\"</strong>\")[1]) if ele not in ('\\n','')]))\n",
    "\n",
    "            for ad in data[1:-1]:\n",
    "\n",
    "                ad_list.append(\", \".join([ele.strip() for ele in re.findall(r'\\>([^\\>]*)\\<', str(ad)) if ele not in ('\\n','')]))\n",
    "\n",
    "            part1 = \" and \".join(ad_list)\n",
    "\n",
    "            #print(part1)\n",
    "\n",
    "            part2 = \" \".join([ele.strip() for ele in re.findall(r'\\>([^\\>]*)\\<', str(data[-1])) if ele not in ('\\n','')])\n",
    "            try:\n",
    "                string = part1+\" \"+part2\n",
    "\n",
    "\n",
    "                address = string.split(\"COUNTY:\")[0]\n",
    "\n",
    "                country = string.split(\"COUNTY:\")[1].split(\"# AFFECTED:\")[0]\n",
    "\n",
    "                string = string.replace(\"LAYOFF EFFECTIVE DATES:\", \"EFFECTIVE DATE:\")\n",
    "\n",
    "                employees_affected = string.split(\"COUNTY:\")[1].split(\"# AFFECTED:\")[1].split(\"EFFECTIVE DATE:\")[0]\n",
    "\n",
    "\n",
    "\n",
    "                effective_date = string.split(\"COUNTY:\")[1].split(\"# AFFECTED:\")[1].split(\"EFFECTIVE DATE:\")[1].split(\"CLOSURE OR LAYOFF:\")[0]\n",
    "\n",
    "                string = string.replace(\"CLOSING OR LAYOFF:\", \"CLOSURE OR LAYOFF:\")\n",
    "\n",
    "                closure_or_layoff = string.split(\"COUNTY:\")[1].split(\"# AFFECTED:\")[1].split(\"EFFECTIVE DATE:\")[1].split(\"CLOSURE OR LAYOFF:\")[1]\n",
    "\n",
    "                company_name.append(name)\n",
    "\n",
    "                company_address.append(address)\n",
    "\n",
    "                country_l.append(country)\n",
    "\n",
    "                employees_affected_l.append(employees_affected)\n",
    "\n",
    "                effective_date_l.append(effective_date)\n",
    "\n",
    "                closure_or_layoff_l.append(closure_or_layoff)\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                error_name.append(name)\n",
    "                error_link.append(url)\n",
    "                \n",
    "    #create dataframe\n",
    "    d1 = {'company_name': company_name, 'company_address': company_address, 'country': country_l, 'employees_affected': employees_affected_l, 'effective_date': effective_date_l, 'closure_or_layoff': closure_or_layoff_l}\n",
    "    df1 = pd.DataFrame(data=d1)\n",
    "    \n",
    "    #create error dataframe\n",
    "    d2 = {'links': error_link, 'name': error_name}\n",
    "    df2 = pd.DataFrame(data=d2)\n",
    "    df2 = df2.drop_duplicates()\n",
    "    print(url.split('/')[-1].split('.')[0]+' is done!')\n",
    "    return df1, df2\n",
    "df_lists = []\n",
    "df_error_lists = []\n",
    "for ele in month_links[:14]:\n",
    "    try:\n",
    "        result = get_df(ele)\n",
    "        df_lists.append(result[0])\n",
    "        df_error_lists.append(result[1])\n",
    "    except:\n",
    "        print(ele)\n",
    "df = pd.concat(df_lists)\n",
    "df.to_csv(\"final_result.csv\", index = False)\n",
    "df = pd.concat(df_error_lists)\n",
    "df.to_csv(\"final_error.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
